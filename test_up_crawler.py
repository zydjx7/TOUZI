#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•Bç«™UPä¸»è§†é¢‘çˆ¬å–
"""

import asyncio
import sys
import os
import logging
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.crawler import BilibiliCrawler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

async def test_up_videos(uid: str, up_name: str, max_videos: int = 10):
    """æµ‹è¯•è·å–æŒ‡å®šUPä¸»çš„è§†é¢‘"""
    crawler = BilibiliCrawler()
    
    try:
        await crawler.init_session()
        
        logger.info(f"ğŸ¯ å¼€å§‹çˆ¬å–UPä¸»: {up_name} (UID: {uid})")
        logger.info("="*60)
        
        # 1. è·å–UPä¸»çš„è§†é¢‘åˆ—è¡¨
        logger.info(f"ğŸ“º è·å–UPä¸»è§†é¢‘åˆ—è¡¨ï¼ˆæœ€å¤š{max_videos}ä¸ªï¼‰...")
        videos = await crawler.get_user_videos(uid, page_size=max_videos)
        
        if not videos:
            logger.error(f"âŒ æœªèƒ½è·å–åˆ°UPä¸» {up_name} çš„è§†é¢‘åˆ—è¡¨")
            return False
        
        logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(videos)} ä¸ªè§†é¢‘")
        
        # æ˜¾ç¤ºè§†é¢‘åˆ—è¡¨
        logger.info("\nğŸ“‹ è§†é¢‘åˆ—è¡¨æ¦‚è§ˆ:")
        for i, video in enumerate(videos[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            title = video.get('title', 'Unknown')
            bvid = video.get('bvid', 'Unknown')
            play_count = video.get('play', 0)
            pubdate = video.get('created', 0)
            
            # è½¬æ¢æ—¶é—´æˆ³
            try:
                pub_time = datetime.fromtimestamp(pubdate).strftime('%Y-%m-%d %H:%M')
            except:
                pub_time = "æœªçŸ¥æ—¶é—´"
            
            logger.info(f"   {i}. {title}")
            logger.info(f"      BVID: {bvid} | æ’­æ”¾é‡: {play_count:,} | å‘å¸ƒ: {pub_time}")
        
        if len(videos) > 5:
            logger.info(f"   ... è¿˜æœ‰ {len(videos) - 5} ä¸ªè§†é¢‘")
        
        # 2. è·å–ç¬¬ä¸€ä¸ªè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯
        if videos:
            first_video = videos[0]
            bvid = first_video.get('bvid', '')
            
            if bvid:
                logger.info(f"\nğŸ” è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯: {first_video.get('title', 'Unknown')}")
                video_info = await crawler.get_video_info(bvid)
                
                if video_info:
                    logger.info("âœ… è§†é¢‘è¯¦ç»†ä¿¡æ¯:")
                    logger.info(f"   æ ‡é¢˜: {video_info.get('title', 'Unknown')}")
                    logger.info(f"   æè¿°: {video_info.get('desc', 'Unknown')[:100]}...")
                    logger.info(f"   æ—¶é•¿: {video_info.get('duration', 0)} ç§’")
                    logger.info(f"   ç‚¹èµæ•°: {video_info.get('stat', {}).get('like', 0):,}")
                    logger.info(f"   æŠ•å¸æ•°: {video_info.get('stat', {}).get('coin', 0):,}")
                    logger.info(f"   æ”¶è—æ•°: {video_info.get('stat', {}).get('favorite', 0):,}")
                    logger.info(f"   åˆ†äº«æ•°: {video_info.get('stat', {}).get('share', 0):,}")
                
                # 3. å°è¯•è·å–è§†é¢‘è½¬å½•/å­—å¹•
                logger.info(f"\nğŸ“ è·å–è§†é¢‘è½¬å½•å†…å®¹...")
                transcript = await crawler.get_video_transcript(bvid)
                
                if transcript:
                    logger.info(f"âœ… æˆåŠŸè·å–è½¬å½•å†…å®¹ ({len(transcript)} å­—ç¬¦)")
                    logger.info(f"   å‰200å­—ç¬¦: {transcript[:200]}...")
                else:
                    logger.warning("âš ï¸  æœªèƒ½è·å–åˆ°è§†é¢‘è½¬å½•å†…å®¹")
                
                # 4. è·å–è§†é¢‘è¯„è®º
                logger.info(f"\nğŸ’¬ è·å–è§†é¢‘è¯„è®ºï¼ˆå‰10æ¡ï¼‰...")
                comments = await crawler.get_video_comments(bvid, limit=10)
                
                if comments:
                    logger.info(f"âœ… æˆåŠŸè·å–åˆ° {len(comments)} æ¡è¯„è®º")
                    for i, comment in enumerate(comments[:3], 1):
                        content = comment.get('content', '').replace('\n', ' ')[:50]
                        author = comment.get('author', 'Unknown')
                        likes = comment.get('like_count', 0)
                        logger.info(f"   {i}. {author}: {content}... (ğŸ‘ {likes})")
                else:
                    logger.warning("âš ï¸  æœªèƒ½è·å–åˆ°è§†é¢‘è¯„è®º")
        
        # 5. ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        logger.info(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶...")
        data = {
            'up_info': {
                'uid': uid,
                'name': up_name,
                'crawl_time': datetime.now().isoformat()
            },
            'videos': videos,
            'sample_video_detail': video_info if 'video_info' in locals() else None,
            'sample_transcript': transcript if 'transcript' in locals() else None,
            'sample_comments': comments if 'comments' in locals() else None
        }
        
        filename = f"data/up_{uid}_{up_name}_videos.json"
        os.makedirs("data", exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False
    finally:
        await crawler.close_session()

async def test_multiple_ups():
    """æµ‹è¯•å¤šä¸ªUPä¸»çš„æ•°æ®è·å–"""
    from config import config
    
    logger.info("ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•UPä¸»è§†é¢‘çˆ¬å–...")
    
    # ä»é…ç½®æ–‡ä»¶è·å–UPä¸»åˆ—è¡¨
    up_list = config.UP_LIST
    
    results = {}
    
    for up in up_list[:2]:  # åªæµ‹è¯•å‰2ä¸ªUPä¸»
        uid = up['uid']
        name = up['name']
        
        logger.info(f"\n{'='*60}")
        success = await test_up_videos(uid, name, max_videos=5)
        results[name] = success
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        await asyncio.sleep(3)
    
    # è¾“å‡ºæ€»ç»“
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ“Š æ‰¹é‡çˆ¬å–ç»“æœæ€»ç»“:")
    for name, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        logger.info(f"   {name}: {status}")

async def main():
    """ä¸»å‡½æ•°"""
    print("è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. æµ‹è¯•å•ä¸ªUPä¸» - æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´")
    print("2. æµ‹è¯•é…ç½®æ–‡ä»¶ä¸­çš„å¤šä¸ªUPä¸»")
    
    try:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1æˆ–2): ").strip()
        
        if choice == "1":
            # æµ‹è¯•æŒ‡å®šçš„UPä¸»
            uid = "1039025435"
            name = "æˆ˜å›½æ—¶ä»£_å§œæ±½æ°´"
            await test_up_videos(uid, name, max_videos=10)
            
        elif choice == "2":
            # æµ‹è¯•å¤šä¸ªUPä¸»
            await test_multiple_ups()
            
        else:
            logger.info("æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤æµ‹è¯•æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´")
            uid = "1039025435"
            name = "æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´"
            await test_up_videos(uid, name, max_videos=10)
            
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 