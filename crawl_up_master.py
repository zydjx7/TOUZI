#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„UPä¸»æ•°æ®çˆ¬å–è„šæœ¬
"""

import asyncio
import sys
import os
import logging
import json
from datetime import datetime
from typing import Optional, Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.crawler import BilibiliCrawler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class UPMasterCrawler:
    """UPä¸»æ•°æ®çˆ¬å–å™¨"""
    
    def __init__(self, delay_between_requests: int = 10):
        self.crawler = BilibiliCrawler()
        self.delay = delay_between_requests
        
    async def get_up_basic_info(self, uid: str) -> Optional[Dict]:
        """è·å–UPä¸»åŸºæœ¬ä¿¡æ¯"""
        url = "https://api.bilibili.com/x/space/acc/info"
        params = {'mid': uid}
        
        logger.info(f"ğŸ“‹ è·å–UPä¸»åŸºæœ¬ä¿¡æ¯ (UID: {uid})...")
        result = await self.crawler._make_request(url, params)
        
        if result and result.get('code') == 0:
            return result.get('data', {})
        else:
            logger.warning(f"è·å–UPä¸»åŸºæœ¬ä¿¡æ¯å¤±è´¥: {result}")
            return None
    
    async def get_up_videos_by_search(self, up_name: str, max_videos: int = 20) -> List[Dict]:
        """é€šè¿‡æœç´¢è·å–UPä¸»è§†é¢‘"""
        logger.info(f"ğŸ” é€šè¿‡æœç´¢è·å–UPä¸» '{up_name}' çš„è§†é¢‘...")
        await asyncio.sleep(self.delay)
        
        videos = await self.crawler.search_videos(up_name, page_size=max_videos)
        
        # è¿‡æ»¤å‡ºè¯¥UPä¸»çš„è§†é¢‘
        filtered_videos = []
        for video in videos:
            if up_name in video.get('author', ''):
                filtered_videos.append(video)
        
        logger.info(f"æ‰¾åˆ° {len(filtered_videos)} ä¸ªè¯¥UPä¸»çš„è§†é¢‘")
        return filtered_videos
    
    async def get_up_dynamics(self, uid: str) -> List[Dict]:
        """è·å–UPä¸»åŠ¨æ€"""
        logger.info(f"ğŸ“± è·å–UPä¸»åŠ¨æ€ (UID: {uid})...")
        await asyncio.sleep(self.delay)
        
        dynamics = await self.crawler.get_user_dynamics(uid)
        return dynamics
    
    async def get_video_details(self, bvid: str) -> Optional[Dict]:
        """è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯"""
        logger.info(f"ğŸ¬ è·å–è§†é¢‘è¯¦æƒ… (BVID: {bvid})...")
        await asyncio.sleep(self.delay)
        
        return await self.crawler.get_video_info(bvid)
    
    async def crawl_up_complete_data(self, uid: str, up_name: str = None) -> Dict:
        """çˆ¬å–UPä¸»å®Œæ•´æ•°æ®"""
        try:
            await self.crawler.init_session()
            
            logger.info(f"ğŸ¯ å¼€å§‹çˆ¬å–UPä¸»å®Œæ•´æ•°æ® (UID: {uid})")
            logger.info("="*60)
            
            # 1. è·å–åŸºæœ¬ä¿¡æ¯
            basic_info = await self.get_up_basic_info(uid)
            if not basic_info:
                logger.error("æ— æ³•è·å–UPä¸»åŸºæœ¬ä¿¡æ¯ï¼Œåœæ­¢çˆ¬å–")
                return {}
            
            actual_name = basic_info.get('name', up_name or 'Unknown')
            logger.info(f"âœ… UPä¸»: {actual_name}")
            logger.info(f"   ç²‰ä¸æ•°: {basic_info.get('follower', 0):,}")
            logger.info(f"   ç­‰çº§: {basic_info.get('level', 'Unknown')}")
            
            # 2. é€šè¿‡æœç´¢è·å–è§†é¢‘
            videos = await self.get_up_videos_by_search(actual_name, max_videos=15)
            
            # 3. è·å–åŠ¨æ€
            dynamics = await self.get_up_dynamics(uid)
            
            # 4. è·å–ç¬¬ä¸€ä¸ªè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
            sample_video_detail = None
            if videos:
                first_video = videos[0]
                bvid = first_video.get('bvid', '')
                if bvid:
                    sample_video_detail = await self.get_video_details(bvid)
            
            # 5. æ•´ç†æ•°æ®
            result = {
                'crawl_info': {
                    'uid': uid,
                    'crawl_time': datetime.now().isoformat(),
                    'crawl_method': 'search_based'  # æ ‡è®°ä½¿ç”¨æœç´¢æ–¹å¼è·å–
                },
                'basic_info': basic_info,
                'videos': videos,
                'dynamics': dynamics,
                'sample_video_detail': sample_video_detail,
                'statistics': {
                    'videos_found': len(videos),
                    'dynamics_found': len(dynamics)
                }
            }
            
            logger.info("\nğŸ“Š çˆ¬å–ç»“æœç»Ÿè®¡:")
            logger.info(f"   åŸºæœ¬ä¿¡æ¯: {'âœ…' if basic_info else 'âŒ'}")
            logger.info(f"   è§†é¢‘æ•°é‡: {len(videos)}")
            logger.info(f"   åŠ¨æ€æ•°é‡: {len(dynamics)}")
            logger.info(f"   è§†é¢‘è¯¦æƒ…ç¤ºä¾‹: {'âœ…' if sample_video_detail else 'âŒ'}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {}
        finally:
            await self.crawler.close_session()
    
    async def save_data(self, data: Dict, filename: str = None):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if not data:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        if not filename:
            uid = data.get('crawl_info', {}).get('uid', 'unknown')
            name = data.get('basic_info', {}).get('name', 'unknown')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"data/up_{uid}_{name}_{timestamp}.json"
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ UPä¸»æ•°æ®çˆ¬å–å·¥å…·")
    logger.info("="*50)
    
    # é…ç½®
    uid = "1039025435"
    up_name = "æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´"
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆ15ç§’å»¶è¿Ÿï¼Œæ›´ä¿å®ˆï¼‰
    crawler = UPMasterCrawler(delay_between_requests=15)
    
    # çˆ¬å–æ•°æ®
    logger.info(f"å¼€å§‹çˆ¬å–UPä¸»: {up_name}")
    data = await crawler.crawl_up_complete_data(uid, up_name)
    
    if data:
        # ä¿å­˜æ•°æ®
        await crawler.save_data(data)
        
        logger.info("\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        logger.info("ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°dataç›®å½•")
        logger.info("\nğŸ’¡ è·å–çš„æ•°æ®åŒ…æ‹¬:")
        logger.info("   âœ“ UPä¸»åŸºæœ¬ä¿¡æ¯ï¼ˆå§“åã€ç²‰ä¸æ•°ã€ç­‰çº§ç­‰ï¼‰")
        logger.info("   âœ“ è§†é¢‘åˆ—è¡¨ï¼ˆé€šè¿‡æœç´¢è·å–ï¼‰")
        logger.info("   âœ“ ç”¨æˆ·åŠ¨æ€")
        logger.info("   âœ“ ç¤ºä¾‹è§†é¢‘è¯¦æƒ…")
        
    else:
        logger.error("ğŸ˜ çˆ¬å–å¤±è´¥ï¼")
        logger.info("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        logger.info("   1. ç½‘ç»œé—®é¢˜")
        logger.info("   2. Bç«™APIé™åˆ¶")
        logger.info("   3. Cookieå¤±æ•ˆ")

if __name__ == "__main__":
    asyncio.run(main()) 