#!/usr/bin/env python3
"""
è´¢ç»æ™ºèƒ½åˆ†æç³»ç»Ÿ - ä¸»ç¨‹åºå…¥å£
Financial Intelligence Analysis System - Main Entry Point
"""

import asyncio
import logging
import signal
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config import config
from src.core.database import DatabaseManager
from src.core.analyzer import ContentAnalyzer
from src.core.crawler import BilibiliCrawler
from src.core.news_aggregator import NewsAggregator
from src.core.report_generator import ReportGenerator
from src.utils.email_notifier import EmailNotifier

# é…ç½®æ—¥å¿—
def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_config = config.LOG_CONFIG
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    Path(log_config['file']).parent.mkdir(parents=True, exist_ok=True)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=getattr(logging, log_config['level']),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_config['file'], encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    return logger

class FinancialAnalysisSystem:
    """è´¢ç»æ™ºèƒ½åˆ†æç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.running = False
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db_manager = DatabaseManager(config.DATABASE_PATH)
        self.crawler = BilibiliCrawler()
        self.analyzer = ContentAnalyzer()
        self.news_aggregator = NewsAggregator()
        self.report_generator = ReportGenerator(self.db_manager)
        self.email_notifier = EmailNotifier(
            **config.EMAIL_CONFIG
        ) if config.EMAIL_CONFIG['email'] else None
        
        self.logger.info("è´¢ç»æ™ºèƒ½åˆ†æç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    async def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        self.logger.info("ğŸš€ å¯åŠ¨è´¢ç»æ™ºèƒ½åˆ†æç³»ç»Ÿ...")
        self.running = True
        
        # åˆå§‹åŒ–çˆ¬è™«ä¼šè¯
        await self.crawler.init_session()
        
        try:
            # å¯åŠ¨ä¸»å¾ªç¯
            await self.main_loop()
        except asyncio.CancelledError:
            self.logger.info("ç³»ç»Ÿæ”¶åˆ°åœæ­¢ä¿¡å·")
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿè¿è¡Œå‡ºé”™: {e}")
            raise
        finally:
            await self.cleanup()
    
    async def main_loop(self):
        """ä¸»è¦ä¸šåŠ¡å¾ªç¯"""
        self.logger.info("å¼€å§‹ä¸»è¦ä¸šåŠ¡å¾ªç¯")
        
        while self.running:
            try:
                # æ‰§è¡Œä¸€è½®åˆ†æ
                await self.run_analysis_cycle()
                
                # ç­‰å¾…ä¸‹ä¸€è½®
                await asyncio.sleep(300)  # 5åˆ†é’Ÿé—´éš”
                
            except Exception as e:
                self.logger.error(f"åˆ†æå¾ªç¯å‡ºé”™: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•
    
    async def run_analysis_cycle(self):
        """æ‰§è¡Œä¸€è½®å®Œæ•´çš„åˆ†æå‘¨æœŸ"""
        self.logger.info("å¼€å§‹æ–°çš„åˆ†æå‘¨æœŸ")
        
        # 1. çˆ¬å–UPä¸»å†…å®¹
        for up_info in config.UP_LIST:
            if not self.running:
                break
                
            try:
                await self.crawl_up_content(up_info['uid'], up_info['name'])
            except Exception as e:
                self.logger.error(f"çˆ¬å–UPä¸» {up_info['name']} å†…å®¹å¤±è´¥: {e}")
        
        # 2. çˆ¬å–æ–°é—»
        try:
            await self.crawl_news()
        except Exception as e:
            self.logger.error(f"çˆ¬å–æ–°é—»å¤±è´¥: {e}")
        
        # 3. åˆ†æå†…å®¹
        try:
            await self.analyze_content()
        except Exception as e:
            self.logger.error(f"åˆ†æå†…å®¹å¤±è´¥: {e}")
        
        # 4. ç”ŸæˆæŠ¥å‘Šï¼ˆæ¯å°æ—¶ä¸€æ¬¡ï¼‰
        import time
        if int(time.time()) % 3600 < 300:  # æ¯å°æ—¶çš„å‰5åˆ†é’Ÿ
            try:
                await self.generate_and_send_report()
            except Exception as e:
                self.logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        
        self.logger.info("åˆ†æå‘¨æœŸå®Œæˆ")
    
    async def crawl_up_content(self, uid: str, up_name: str):
        """çˆ¬å–UPä¸»å†…å®¹"""
        self.logger.info(f"å¼€å§‹çˆ¬å–UPä¸»å†…å®¹: {up_name}")
        
        # è·å–è§†é¢‘åˆ—è¡¨
        videos = await self.crawler.get_user_videos(uid)
        
        for video in videos[:10]:  # åªå¤„ç†æœ€æ–°çš„10ä¸ªè§†é¢‘
            try:
                # è·å–è¯¦ç»†ä¿¡æ¯
                video_info = await self.crawler.get_video_info(video['bvid'])
                if not video_info:
                    continue
                
                # è·å–è½¬å½•æ–‡æœ¬
                transcript = await self.crawler.get_video_transcript(video['bvid'])
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                from src.core.database import VideoContent
                import hashlib
                from datetime import datetime
                
                content_hash = hashlib.md5(
                    (video_info['title'] + video_info['desc']).encode()
                ).hexdigest()
                
                video_content = VideoContent(
                    bvid=video['bvid'],
                    title=video_info['title'],
                    description=video_info['desc'],
                    transcript=transcript,
                    publish_time=datetime.fromtimestamp(video_info['pubdate']),
                    up_name=up_name,
                    view_count=video_info['stat']['view'],
                    like_count=video_info['stat']['like'],
                    coin_count=video_info['stat']['coin'],
                    share_count=video_info['stat']['share'],
                    tags=[tag['tag_name'] for tag in video_info.get('tags', [])],
                    content_hash=content_hash
                )
                
                self.db_manager.save_video(video_content)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                await asyncio.sleep(config.CRAWLER_CONFIG['rate_limit_delay'])
                
            except Exception as e:
                self.logger.error(f"å¤„ç†è§†é¢‘ {video.get('bvid', 'unknown')} å¤±è´¥: {e}")
        
        # è·å–åŠ¨æ€
        dynamics = await self.crawler.get_user_dynamics(uid)
        
        for dynamic in dynamics[:20]:  # åªå¤„ç†æœ€æ–°çš„20æ¡åŠ¨æ€
            try:
                from src.core.database import DynamicContent
                
                content_hash = hashlib.md5(
                    dynamic['content'].encode()
                ).hexdigest()
                
                dynamic_content = DynamicContent(
                    dynamic_id=str(dynamic['id']),
                    content=dynamic['content'],
                    publish_time=datetime.fromtimestamp(dynamic['timestamp']),
                    up_name=up_name,
                    like_count=dynamic.get('like_count', 0),
                    forward_count=dynamic.get('forward_count', 0),
                    comment_count=dynamic.get('comment_count', 0),
                    content_hash=content_hash
                )
                
                self.db_manager.save_dynamic(dynamic_content)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†åŠ¨æ€å¤±è´¥: {e}")
    
    async def crawl_news(self):
        """çˆ¬å–æ–°é—»"""
        self.logger.info("å¼€å§‹çˆ¬å–æ–°é—»")
        
        for category in ['financial', 'geopolitical']:
            try:
                news_list = await self.news_aggregator.fetch_latest_news(category)
                
                for news in news_list:
                    self.db_manager.save_news(news)
                    
            except Exception as e:
                self.logger.error(f"çˆ¬å– {category} æ–°é—»å¤±è´¥: {e}")
    
    async def analyze_content(self):
        """åˆ†æå†…å®¹"""
        self.logger.info("å¼€å§‹åˆ†æå†…å®¹")
        
        # è·å–æœ€è¿‘çš„æœªåˆ†æå†…å®¹
        recent_videos = self.db_manager.get_latest_content('video', days=1)
        recent_dynamics = self.db_manager.get_latest_content('dynamic', days=1)
        recent_news = self.db_manager.get_latest_content('news', days=1)
        
        # åˆ†æè§†é¢‘
        for video in recent_videos[:50]:  # é™åˆ¶å¤„ç†æ•°é‡
            await self.analyze_single_content(
                video['bvid'], 'video', 
                f"{video['title']} {video['description']} {video['transcript']}"
            )
        
        # åˆ†æåŠ¨æ€
        for dynamic in recent_dynamics[:100]:
            await self.analyze_single_content(
                dynamic['dynamic_id'], 'dynamic', dynamic['content']
            )
        
        # åˆ†ææ–°é—»
        for news in recent_news[:50]:
            await self.analyze_single_content(
                str(news['id']), 'news', 
                f"{news['title']} {news['content']}"
            )
    
    async def analyze_single_content(self, content_id: str, content_type: str, text: str):
        """åˆ†æå•ä¸ªå†…å®¹"""
        try:
            # æƒ…æ„Ÿåˆ†æ
            sentiment_score = self.analyzer.analyze_sentiment(text)
            
            # æå–å…³é”®ç‚¹
            key_points = self.analyzer.extract_key_points(text)
            
            # æ£€æµ‹æŠ•èµ„ä¿¡å·
            investment_signals = self.analyzer.detect_investment_signals(text)
            
            # è¯„ä¼°é£é™©ç­‰çº§
            risk_level = self.analyzer.assess_risk_level(sentiment_score, investment_signals)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self.analyzer.calculate_confidence(text, investment_signals)
            
            # ä¿å­˜åˆ†æç»“æœ
            from src.core.database import AnalysisResult
            from datetime import datetime
            
            result = AnalysisResult(
                content_id=content_id,
                content_type=content_type,
                sentiment_score=sentiment_score,
                key_points=key_points,
                investment_signals=investment_signals,
                risk_level=risk_level,
                confidence=confidence,
                analysis_time=datetime.now()
            )
            
            self.db_manager.save_analysis_result(result)
            
        except Exception as e:
            self.logger.error(f"åˆ†æå†…å®¹ {content_id} å¤±è´¥: {e}")
    
    async def generate_and_send_report(self):
        """ç”Ÿæˆå¹¶å‘é€æŠ¥å‘Š"""
        self.logger.info("å¼€å§‹ç”ŸæˆæŠ¥å‘Š")
        
        try:
            # ç”ŸæˆæŠ¥å‘Š
            report_content = self.report_generator.generate_daily_report()
            
            # å‘é€é‚®ä»¶
            if self.email_notifier and config.RECIPIENT_EMAIL:
                from datetime import date
                subject = f"è´¢ç»æ™ºèƒ½åˆ†ææ—¥æŠ¥ - {date.today()}"
                self.email_notifier.send_report(
                    config.RECIPIENT_EMAIL, subject, report_content
                )
                self.logger.info("æŠ¥å‘Šå‘é€å®Œæˆ")
            else:
                self.logger.warning("é‚®ä»¶é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡å‘é€")
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæˆ–å‘é€æŠ¥å‘Šå¤±è´¥: {e}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("å¼€å§‹æ¸…ç†èµ„æº...")
        
        try:
            await self.crawler.close_session()
            self.logger.info("æ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        self.logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
        self.running = False

# å…¨å±€ç³»ç»Ÿå®ä¾‹
system = None

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    if system:
        system.stop()

async def main():
    """ä¸»å‡½æ•°"""
    global system
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # åˆ›å»ºå¹¶å¯åŠ¨ç³»ç»Ÿ
        system = FinancialAnalysisSystem()
        await system.start()
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°é”®ç›˜ä¸­æ–­ä¿¡å·")
    except Exception as e:
        logger.error(f"ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
        return 1
    
    logger.info("ç³»ç»Ÿå·²åœæ­¢")
    return 0

if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 