#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆBç«™çˆ¬è™«æµ‹è¯• - å¤„ç†åçˆ¬è™«
"""

import asyncio
import logging
import sys
import time
import random
from pathlib import Path
from datetime import datetime
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.crawler import BilibiliCrawler
from src.core.database import DatabaseManager, VideoContent

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class EnhancedBilibiliCrawler(BilibiliCrawler):
    """å¢å¼ºç‰ˆçˆ¬è™«ï¼Œæ·»åŠ åçˆ¬è™«å¤„ç†"""
    
    def __init__(self):
        super().__init__()
        # å¢åŠ æ›´çœŸå®çš„è¯·æ±‚å¤´
        self.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
        })
        
    async def _make_request_with_retry(self, url: str, params: dict = None, max_retries: int = 3):
        """å¸¦é‡è¯•çš„è¯·æ±‚"""
        for attempt in range(max_retries):
            try:
                # æ·»åŠ éšæœºå»¶è¿Ÿ
                if attempt > 0:
                    delay = random.uniform(3, 5) * (attempt + 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)
                
                result = await self._make_request(url, params)
                
                if result and result.get('code') == 0:
                    return result
                elif result and result.get('code') == -799:
                    logger.warning(f"è¯·æ±‚é¢‘ç¹ï¼Œç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥")
                    continue
                else:
                    return result
                    
            except Exception as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt == max_retries - 1:
                    raise
        
        return None

async def test_with_cookie_approach():
    """ä½¿ç”¨Cookieæ–¹å¼æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ“‹ ä½¿ç”¨Cookieæ–¹å¼è®¿é—®")
    print("="*60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰Cookieé…ç½®
    cookie_file = Path("config/cookies.json")
    if cookie_file.exists():
        print("âœ… å‘ç°Cookieé…ç½®æ–‡ä»¶")
    else:
        print("âŒ æœªæ‰¾åˆ°Cookieé…ç½®")
        print("\nå»ºè®®æ“ä½œï¼š")
        print("1. åœ¨æµè§ˆå™¨ä¸­ç™»å½•Bç«™")
        print("2. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–Cookie")
        print("3. è¿è¡Œ: python src/utils/cookie_helper.py")
        print("4. å°†Cookieç²˜è´´åˆ°æç¤ºä¸­")
        
        # æä¾›å¿«é€Ÿé…ç½®é€‰é¡¹
        cookie = input("\nç°åœ¨é…ç½®Cookieå—ï¼Ÿ(ç›´æ¥ç²˜è´´Cookieæˆ–æŒ‰å›è½¦è·³è¿‡): ").strip()
        if cookie:
            from src.utils.cookie_helper import CookieHelper
            helper = CookieHelper()
            helper.save_bilibili_cookie(cookie)
            print("âœ… Cookieå·²ä¿å­˜")

async def test_with_delays():
    """ä½¿ç”¨å»¶è¿Ÿç­–ç•¥æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ¢ ä½¿ç”¨å»¶è¿Ÿç­–ç•¥æµ‹è¯•")
    print("="*60)
    
    # ç›®æ ‡UPä¸»ä¿¡æ¯
    test_uid = "1039025435"
    up_name = "æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´"
    
    # ä½¿ç”¨å¢å¼ºç‰ˆçˆ¬è™«
    crawler = EnhancedBilibiliCrawler()
    db_manager = DatabaseManager("data/test_with_delays.db")
    
    await crawler.init_session()
    
    try:
        # å…ˆç­‰å¾…ä¸€æ®µæ—¶é—´
        print("â³ ç­‰å¾…5ç§’ä»¥é¿å…é¢‘ç‡é™åˆ¶...")
        await asyncio.sleep(5)
        
        print(f"\nğŸ“¹ è·å–UPä¸»è§†é¢‘: {up_name}")
        
        # ä¿®æ”¹è¯·æ±‚æ–¹å¼ï¼Œä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿ
        crawler.rate_limit_delay = 5  # å¢åŠ åˆ°5ç§’
        
        videos = await crawler._make_request_with_retry(
            "https://api.bilibili.com/x/space/arc/search",
            params={
                'mid': test_uid,
                'ps': 3,  # åªè·å–3ä¸ªè§†é¢‘
                'pn': 1,
                'order': 'pubdate',
                'tid': 0,
                'keyword': '',
                'jsonp': 'jsonp'
            }
        )
        
        if videos and videos.get('code') == 0:
            video_list = videos.get('data', {}).get('list', {}).get('vlist', [])
            print(f"âœ… æˆåŠŸè·å– {len(video_list)} ä¸ªè§†é¢‘")
            
            for i, video in enumerate(video_list):
                print(f"\nè§†é¢‘ {i+1}:")
                print(f"æ ‡é¢˜: {video['title']}")
                print(f"BVå·: {video['bvid']}")
                print(f"æ’­æ”¾é‡: {video['play']:,}")
                
                # è·å–è§†é¢‘è¯¦æƒ…
                print("â³ ç­‰å¾…5ç§’...")
                await asyncio.sleep(5)
                
                video_info = await crawler._make_request_with_retry(
                    "https://api.bilibili.com/x/web-interface/view",
                    params={'bvid': video['bvid']}
                )
                
                if video_info and video_info.get('code') == 0:
                    info = video_info['data']
                    print(f"âœ… è·å–è¯¦æƒ…æˆåŠŸ")
                    print(f"æè¿°: {info['desc'][:100]}...")
                    
                    # å°è¯•è·å–å­—å¹•
                    print("\nğŸ“ å°è¯•è·å–å­—å¹•...")
                    await asyncio.sleep(5)
                    
                    transcript = await crawler.get_video_transcript(video['bvid'])
                    if transcript:
                        print(f"âœ… è·å–åˆ°æ–‡æœ¬å†…å®¹")
                        print(f"é•¿åº¦: {len(transcript)} å­—ç¬¦")
                        print(f"é¢„è§ˆ: {transcript[:150]}...")
                    else:
                        print("âš ï¸  æœªè·å–åˆ°å­—å¹•ï¼Œä½¿ç”¨è§†é¢‘æè¿°")
                        
        else:
            print(f"âŒ è·å–å¤±è´¥: {videos}")
            
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
    finally:
        await crawler.close_session()

async def test_alternative_methods():
    """æµ‹è¯•æ›¿ä»£æ–¹æ¡ˆ"""
    print("\n" + "="*60)
    print("ğŸ”„ æ›¿ä»£æ–¹æ¡ˆå»ºè®®")
    print("="*60)
    
    print("\n1. ä½¿ç”¨æœç´¢APIä»£æ›¿ç”¨æˆ·è§†é¢‘APIï¼š")
    print("   - æœç´¢UPä¸»åç§°è·å–è§†é¢‘")
    print("   - é™åˆ¶è¾ƒå°‘ï¼Œæ›´å®¹æ˜“æˆåŠŸ")
    
    print("\n2. ä½¿ç”¨Webç«¯æ¥å£ï¼š")
    print("   - æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®")
    print("   - éœ€è¦æ›´å¤æ‚çš„è¯·æ±‚å¤´")
    
    print("\n3. ä½¿ç”¨MCPæ–¹æ¡ˆï¼š")
    print("   - Bilibili MCP (éœ€è¦Apify token)")
    print("   - Browser-Tools MCP (è‡ªåŠ¨åŒ–æµè§ˆå™¨)")
    
    print("\n4. æ‰‹åŠ¨æ–¹å¼ï¼š")
    print("   - ä½¿ç”¨å“”å“©å›ç­‰æµè§ˆå™¨æ’ä»¶")
    print("   - æ‰‹åŠ¨ä¸‹è½½åæ‰¹é‡å¤„ç†")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ” Bç«™çˆ¬è™«é—®é¢˜è¯Šæ–­")
    print("="*60)
    
    # 1. å…ˆå°è¯•ä½¿ç”¨å»¶è¿Ÿç­–ç•¥
    await test_with_delays()
    
    # 2. æä¾›Cookieæ–¹æ¡ˆ
    await test_with_cookie_approach()
    
    # 3. æ˜¾ç¤ºæ›¿ä»£æ–¹æ¡ˆ
    await test_alternative_methods()
    
    print("\n" + "="*60)
    print("ğŸ“Œ æ¨èè§£å†³æ–¹æ¡ˆï¼š")
    print("1. çŸ­æœŸï¼šé…ç½®Cookie + å¢åŠ è¯·æ±‚å»¶è¿Ÿ")
    print("2. é•¿æœŸï¼šè€ƒè™‘ä½¿ç”¨MCPæˆ–å…¶ä»–è‡ªåŠ¨åŒ–æ–¹æ¡ˆ")
    print("="*60)

if __name__ == "__main__":
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    Path("data").mkdir(exist_ok=True)
    Path("config").mkdir(exist_ok=True)
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())
