#!/usr/bin/env python3
"""
æµ‹è¯•ç‰¹å®šUPä¸» - æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´
éªŒè¯èƒ½å¦æ­£ç¡®æå–è§†é¢‘æ–‡æœ¬å†…å®¹
"""

import asyncio
import logging
import sys
from pathlib import Path
from datetime import datetime
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.crawler import BilibiliCrawler
from src.core.database import DatabaseManager, VideoContent, DynamicContent

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_specific_up():
    """æµ‹è¯•ç‰¹å®šUPä¸»çš„æ•°æ®è·å–"""
    
    # ç›®æ ‡UPä¸»ä¿¡æ¯
    test_uid = "1039025435"
    up_name = "æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´"
    
    crawler = BilibiliCrawler()
    db_manager = DatabaseManager("data/test_specific_up.db")
    
    await crawler.init_session()
    
    try:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ æµ‹è¯•UPä¸»: {up_name} (UID: {test_uid})")
        print(f"{'='*60}\n")
        
        # 1. è·å–æœ€æ–°è§†é¢‘åˆ—è¡¨
        print("ğŸ“¹ è·å–æœ€æ–°è§†é¢‘...")
        videos = await crawler.get_user_videos(test_uid, page_size=5)
        
        if not videos:
            print("âŒ æœªèƒ½è·å–åˆ°è§†é¢‘åˆ—è¡¨")
            return
            
        print(f"âœ… æˆåŠŸè·å– {len(videos)} ä¸ªè§†é¢‘")
        
        # 2. æµ‹è¯•å‰3ä¸ªè§†é¢‘çš„æ–‡æœ¬æå–
        for i, video in enumerate(videos[:3]):
            print(f"\n{'='*50}")
            print(f"ğŸ“Œ å¤„ç†ç¬¬ {i+1} ä¸ªè§†é¢‘")
            print(f"æ ‡é¢˜: {video.get('title', 'Unknown')}")
            print(f"BVå·: {video.get('bvid', 'Unknown')}")
            
            await asyncio.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # è·å–è§†é¢‘è¯¦æƒ…
            video_info = await crawler.get_video_info(video['bvid'])
            if not video_info:
                print("âŒ æ— æ³•è·å–è§†é¢‘è¯¦æƒ…")
                continue
            
            print(f"æ’­æ”¾é‡: {video_info['stat']['view']:,}")
            print(f"å‘å¸ƒæ—¶é—´: {datetime.fromtimestamp(video_info['pubdate']).strftime('%Y-%m-%d %H:%M:%S')}")
            
            # è·å–è§†é¢‘æ–‡æœ¬ï¼ˆå­—å¹•æˆ–æè¿°ï¼‰
            print("\nğŸ“ æå–è§†é¢‘æ–‡æœ¬...")
            transcript = await crawler.get_video_transcript(video['bvid'])
            
            if transcript:
                print(f"âœ… æˆåŠŸæå–æ–‡æœ¬")
                print(f"æ–‡æœ¬é•¿åº¦: {len(transcript)} å­—ç¬¦")
                print(f"æ–‡æœ¬é¢„è§ˆ: {transcript[:200]}...")
                
                # åˆ¤æ–­æ˜¯å­—å¹•è¿˜æ˜¯æè¿°
                if len(transcript) > len(video_info.get('desc', '')):
                    print("ğŸ“Œ æ¥æº: è§†é¢‘å­—å¹•")
                else:
                    print("ğŸ“Œ æ¥æº: è§†é¢‘æè¿°")
            else:
                print("âš ï¸  æœªèƒ½æå–åˆ°æ–‡æœ¬å†…å®¹")
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            try:
                content_hash = hashlib.md5(
                    (video_info['title'] + video_info['desc']).encode()
                ).hexdigest()
                
                video_content = VideoContent(
                    bvid=video['bvid'],
                    title=video_info['title'],
                    description=video_info['desc'],
                    transcript=transcript or video_info['desc'],  # å¦‚æœæ²¡æœ‰å­—å¹•ï¼Œä½¿ç”¨æè¿°
                    publish_time=datetime.fromtimestamp(video_info['pubdate']),
                    up_name=up_name,
                    view_count=video_info['stat']['view'],
                    like_count=video_info['stat']['like'],
                    coin_count=video_info['stat']['coin'],
                    share_count=video_info['stat']['share'],
                    tags=[tag['tag_name'] for tag in video_info.get('tags', [])],
                    content_hash=content_hash
                )
                
                db_manager.save_video(video_content)
                print("âœ… å·²ä¿å­˜åˆ°æ•°æ®åº“")
                
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            
            await asyncio.sleep(1)
        
        # 3. è·å–æœ€æ–°åŠ¨æ€
        print(f"\n{'='*50}")
        print("ğŸ’¬ è·å–æœ€æ–°åŠ¨æ€...")
        dynamics = await crawler.get_user_dynamics(test_uid)
        
        if dynamics:
            print(f"âœ… æˆåŠŸè·å– {len(dynamics)} æ¡åŠ¨æ€")
            
            # æ˜¾ç¤ºå‰3æ¡åŠ¨æ€
            for i, dynamic in enumerate(dynamics[:3]):
                print(f"\nåŠ¨æ€ {i+1}:")
                print(f"å†…å®¹: {dynamic['content'][:100]}...")
                print(f"ç‚¹èµ: {dynamic['like_count']}")
                
                # ä¿å­˜åŠ¨æ€
                try:
                    content_hash = hashlib.md5(
                        dynamic['content'].encode()
                    ).hexdigest()
                    
                    dynamic_content = DynamicContent(
                        dynamic_id=str(dynamic['id']),
                        content=dynamic['content'],
                        publish_time=datetime.fromtimestamp(dynamic['timestamp']),
                        up_name=up_name,
                        like_count=dynamic.get('like_count', 0),
                        forward_count=dynamic.get('forward_count', 0),
                        comment_count=dynamic.get('comment_count', 0),
                        content_hash=content_hash
                    )
                    
                    db_manager.save_dynamic(dynamic_content)
                    print("âœ… åŠ¨æ€å·²ä¿å­˜")
                    
                except Exception as e:
                    print(f"âŒ ä¿å­˜åŠ¨æ€å¤±è´¥: {e}")
        else:
            print("âŒ æœªèƒ½è·å–åŠ¨æ€")
        
        # 4. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
        print(f"{'='*60}")
        
        stats = db_manager.get_statistics()
        print(f"æ€»è§†é¢‘æ•°: {stats.get('total_videos', 0)}")
        print(f"æ€»åŠ¨æ€æ•°: {stats.get('total_dynamics', 0)}")
        
        # 5. éªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®
        print(f"\nğŸ“„ æ•°æ®åº“ä¸­çš„è§†é¢‘å†…å®¹ç¤ºä¾‹ï¼š")
        saved_videos = db_manager.get_latest_content('video', up_name=up_name)
        if saved_videos:
            for video in saved_videos[:2]:
                print(f"\næ ‡é¢˜: {video['title']}")
                print(f"æ–‡æœ¬å†…å®¹: {video['transcript'][:150]}...")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
    
    finally:
        await crawler.close_session()
        print(f"\n{'='*60}")
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: data/test_specific_up.db")
        print(f"{'='*60}")

if __name__ == "__main__":
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    Path("data").mkdir(exist_ok=True)
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_specific_up())
