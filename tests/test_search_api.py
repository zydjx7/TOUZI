#!/usr/bin/env python3
"""
ä½¿ç”¨Bç«™æœç´¢APIæµ‹è¯• - ç»•è¿‡ç”¨æˆ·è§†é¢‘APIé™åˆ¶
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.crawler import BilibiliCrawler

async def test_search_api():
    """ä½¿ç”¨æœç´¢APIè·å–UPä¸»è§†é¢‘"""
    crawler = BilibiliCrawler()
    await crawler.init_session()
    
    try:
        # é€šè¿‡æœç´¢UPä¸»åå­—è·å–è§†é¢‘
        up_name = "æˆ˜å›½æ—¶ä»£_å§œæ±æ±½æ°´"
        print(f"ğŸ” æœç´¢UPä¸»: {up_name}")
        
        # ä½¿ç”¨æœç´¢åŠŸèƒ½
        videos = await crawler.search_videos(up_name, page_size=10)
        
        if videos:
            print(f"\nâœ… æœç´¢åˆ° {len(videos)} ä¸ªç›¸å…³è§†é¢‘")
            
            # ç­›é€‰è¯¥UPä¸»çš„è§†é¢‘
            up_videos = [v for v in videos if up_name in v.get('author', '')]
            print(f"âœ… å…¶ä¸­ {len(up_videos)} ä¸ªæ¥è‡ªç›®æ ‡UPä¸»")
            
            for i, video in enumerate(up_videos[:3]):
                print(f"\nè§†é¢‘ {i+1}:")
                print(f"æ ‡é¢˜: {video['title']}")
                print(f"ä½œè€…: {video['author']}")
                print(f"BVå·: {video['bvid']}")
                print(f"æ’­æ”¾é‡: {video.get('play', 0)}")
                
                # å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                await asyncio.sleep(3)
                
                # è·å–è§†é¢‘è¯¦æƒ…
                print("\nè·å–è§†é¢‘è¯¦æƒ…...")
                video_info = await crawler.get_video_info(video['bvid'])
                
                if video_info:
                    print(f"âœ… æè¿°: {video_info['desc'][:100]}...")
                    
                    # è·å–å­—å¹•
                    await asyncio.sleep(3)
                    transcript = await crawler.get_video_transcript(video['bvid'])
                    
                    if transcript:
                        print(f"âœ… æ–‡æœ¬é•¿åº¦: {len(transcript)} å­—ç¬¦")
                        print(f"æ–‡æœ¬é¢„è§ˆ: {transcript[:150]}...")
                    else:
                        print("âš ï¸  æ— å­—å¹•ï¼Œä»…æœ‰è§†é¢‘æè¿°")
        else:
            print("âŒ æœç´¢å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    finally:
        await crawler.close_session()

if __name__ == "__main__":
    print("ä½¿ç”¨æœç´¢APIæµ‹è¯•ï¼ˆé¿å¼€ç”¨æˆ·APIé™åˆ¶ï¼‰")
    print("="*50)
    asyncio.run(test_search_api())
